{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到了这一步 神经元什么的就算是有个比较感性的认识了。那么问题来了。这个人工智能有什么关系。目前来说能算出正确的结果都还是因为有个莫名的神奇力量设置了神奇的权重。问题是如何设置这个权重呢？\n",
    "首先数据输入要做一些修改：增加了一列价格数据\n",
    "\n",
    "城市x1 区x2 户型x3 地铁距离x4 学区房x5 价格y\n",
    "12     3    2      1000       0         8000\n",
    "13     2    3      2000       0         9100\n",
    "。。。。\n",
    "\n",
    "\n",
    "然后我们把权重W随机初始化一个数。$\\theta$初始化为0. \n",
    "然后一顿算，得出了一堆数。那么我们如何评估自己的猜测好不好呢？这里就要引出损失函数（loss function）的概念了。 这就像考试，我们蒙的越准，那么屁股上挨的棍子就越少。\n",
    "一种可能的损失函数是求误差的和。但仔细想想不太好，因为如果一部分猜测多猜了1000 另外一部分少猜了1000 这个函数会认为我们猜的很准。稍微改进一下可以是误差绝对值的和。针对于这个例子。其实还不错。当然损失函数有很多。他们有哪些优缺点，如何选择，也是一个不小的话题。后面我会单独讲[link:loss_function]。现在浅尝辄止。\n",
    "\n",
    "有了损失函数，现在的问题就可以以一个比较数学（非人话）的方式描述了，就是求 L(Y-f(x))的最小值。翻译过来就是为了少挨揍，就得考好点。\n",
    "但怎么考好呢？ 有一句话叫做失败是成功之母。我们会从失败中吸取教训。比如我们来算一下根号7是多少。一开始我们猜是2,2^2是4小了，就往大了猜，猜3 一看又大了。那猜2.5 发现又小了。不断根据误差更新自己的猜测。最终我们猜出了一个精确度可以忍受的答案。就如同大多数人的理想型都牵着别人的手（我比较幸运）。 好我们也模拟一下这个过程。\n",
    "比如 我们一顿算，损失函数说我们多猜了1000. 那这多出了这1000 是哪多的呢？ 谁来背这个锅？首当其冲的当然是激活函数。 这里会用到导数，导数描述了一个函数的输出是如何根据输入量的变化而变化的。 当然是每个权重都有份。怎么分最合理呢？当然是按照权重比分。比如一个权重是1 一个是3.那很明显3的那个权重是老鼠屎的可能性最大。着重铲它。虽然挺美好的。但是计算挺繁琐的，wi/Ew 算每个权重，而且每次占比都要重新计算，所以就取巧，计算Wt。通常来说，在更新矩阵前会把误差乘以一个相对小的系数（learning_rate) 防止权重更新过大导致的震荡。 更新每一层都是同样的原理。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
