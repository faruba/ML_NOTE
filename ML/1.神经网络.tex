神经元。
一讲到神经网络就必然会讲到神经元， 输入向量，权重矩阵，激活函数。然后说 a = g(WX+b). 然而一直挥之不去的问题在我脑中存在了很久，这些运算的意义是什么？
鉴于这个问题给我的痛苦，我想要先回答这个问题。然后很多事情就不言自明了。

举个例子，如果我问 一个房子多少钱。 你可能会反过来问我很多问题：哪个城市，哪个区，什么户型，离地铁多远，是学区房吗等等。
那么这些问题，代表着一些特性（feature）每个特性有对应的取值范围（值域）
如果我想用程序估算价格，那么为了方便管理。我们可以给这些特性命名为 x1，x2... xn.为了方便计算，我们把数据做一些处理。
比如 把城市按照发达程度倒排序，然后依次设定值为0,1,2,3这样的数字。区，户型也都用相似的方式处理。是否是学区房用1代表是，用0代表不是. 离地铁距离就精确到米的最短距离
然后输入可能就变成了如下表格
城市x1 区x2 户型x3 地铁距离x4 学区房x5
12     3    2      1000       0
哈尔滨 道外区 2室1厅 
。。。。
即便人和电脑的距离只有一个屏幕。但这种格式也不是给人看的。
把一行拿出来作为一个数组，就是输入向量了，相当于公式里面的X. 
（我不懂房，以下都是瞎说，不代表真实房价更不能作为买房的依据）
比如一个房子的价格，在哪个城市很重要。一样的户型，在铁岭和在深圳差别可是很大的。所以在城市这个项上，我会给他一个很大的权重。由于我们按发达程度倒排序，也就是越繁荣，值越大 所以这个权重（为了方便和x1对应起名为w1)会是一个比较大的数，比如4000。 假如深圳的值是20. 那么20 * 4000 = 80000 意味着深圳均价每平米8万元（黑的漂亮）。
而距离地铁站越远，价格越便宜，那么这个权重（w4）就是一个负值，比如 -0.1 代表每远一公里就每平米便宜100元。
假设有一股神奇的力量已经帮我们设置好这些权重了[4000, 1000, 500, -0.1, 1500] 那么对于 输入是【12     3    2      1000       0】的房子。价格就是 4000*12 + 1000*3+500*2+1000*-1+0*1500 = 自己算。
总结一下 权重（weight 也就是公式中的W） 他描述了 每个特性对我们关心的问题的结果产生怎样的影响。 把输入和对应的权重相乘再加起来，就可以得出房子的价格的估计了
如果你了解线性代数。发现用矩阵表示相当简洁 x1*w1+x2*w2+...+xn*wn => WX 其中 w = [4000, 1000, 500, -0.1, 1500] X= 【12     3    2      1000       0】t

由于我们对输入数据的处理方式，决定了类似 [0,0,1000, 0] 这样的数据预测出相当假的数据（-100） 这时候 偏置（b) 就很有用处了。他提供了一个基准量。就好比我不能喝酒这句话，在东北的意思一顿只能喝4,5瓶啤酒。在我这就是半瓶RIO。

好现在该说激活函数g了。它算干嘛滴呢？他有很多功能。比如可以用来处理最后的输出。举个例子。比如判断一个图片是不是猫，如果没有用激活函数，得到的答案可能是8983. 用了激活函数就可能是 0.95 它的意思是 是猫的可能性是 95%。
另外一个作用是引入非线性特性。我当时看到这句话时，是很不开心的，什么意思？为什么非要引入非线性？不引人会怎样？ 答案是不引人，不论多深的网络，都可以根据矩阵的线性操作合成一个矩阵。如果一层神经网络就搞定了，也就没后面深度学习什么事了。
所以一定要引入。翻译一下就是：一顿操作猛如虎一看比分0杠5. 虽然书上都这么解释，但我还是觉得不好接受：我虽然承认引入非线性的必要性，但是非线性到底起了什么样的作用？非线性函数那么多，什么函数都可以吗？探究这个问题需要很精深的数学功底。如果我没有理解错的话，相当广的非线性函数族都是可以的。但是考虑到 正向逆向（求导）的计算成本，值域范围，输出范围，经得住人们实验结果考验的 就是书中介绍的那些激活函数了。
后来我知道了一个名词叫做非线性变换才真正理解激活函数的作用。比如笛卡尔坐标系转换到极坐标系。可能在笛卡尔坐标系中很复杂的图形转成极坐标之后，就是一条直线。问题瞬间就简单了。就好比男人世界中，女朋友想表达“你不关心我了，整天就知道玩你那个破游戏，你已经连着2个月零13个小时没有说爱我了，明天就是情人节了我明显知道你忘了，怎么暗示你都不开窍。。。。。” 这个意思她只需要说一句话“我们分手吧”


到目前为止我们只看到了一个神经元。如果我现在画了很多个这样的圈圈并排成一列，你能感受到我的深刻的用意（诅咒）吗？我在干嘛？结合上面的讲解。每个圈圈的输入一样，但是他们却可以有不同的权重，这就是说他们每一个圈都可以对同一个数据做不同的理解。
比如第一个圈可以预测房子值多少钱，第二个圈可以推测适不适合做投资。第三个圈拆了后能拿多少钱等等 。现在这个W长什么样子了呢。变成了一个矩阵。每一行代表一个神经元的权重向量。然后感谢一下线代的优美，公式依旧是哪个公式。


既然这个也能理解了。那么再加几列，变成下面这种剪不断理还乱的样子会怎样呢？如果已经理解上面的内容。其实不论加多少列都是一个道理。每一层接受上一次的输入，处理成为结果，传递下去。这样信息随着层数的加深，含义就随之变得更加抽象
比如输入是 城市 户型这样的信息，后面一层就有了，价格，拆迁所得等概念，再往后可能就是 半夜笑醒的概率，买泰坦显卡的心痛指数等等。

这时候公式变成什么样子了呢？ 稍微有点吓人 但很有规律 a = gn(WnXn+bn) ,Xn = gn-1(Wn-1Xn-1+bn-1)... 

到了这一步 神经元什么的就算是有个比较感性的认识了。那么问题来了。这个人工智能有什么关系。目前来说能算出正确的结果都还是因为有个莫名的神奇力量设置了神奇的权重。问题是如何设置这个权重呢？
首先数据输入要做一些修改：增加了一列价格数据

城市x1 区x2 户型x3 地铁距离x4 学区房x5 价格y
12     3    2      1000       0         8000
13     2    3      2000       0         9100
。。。。


然后我们把权重W随机初始化一个数。b初始化为0. 
然后一顿算，得出了一堆数。那么我们如何评估自己的猜测好不好呢？这里就要引出损失函数（loss function）的概念了。 这就像考试，我们蒙的越准，那么屁股上挨的棍子就越少。
一种可能的损失函数是求误差的和。但仔细想想不太好，因为如果一部分猜测多猜了1000 另外一部分少猜了1000 这个函数会认为我们猜的很准。稍微改进一下可以是误差绝对值的和。针对于这个例子。其实还不错。当然损失函数有很多。他们有哪些优缺点，如何选择，也是一个不小的话题。后面我会单独讲[link:loss_function]。现在浅尝辄止。

有了损失函数，现在的问题就可以以一个比较数学（非人话）的方式描述了，就是求 L(Y-f(x))的最小值。翻译过来就是为了少挨揍，就得考好点。
但怎么考好呢？ 有一句话叫做失败是成功之母。我们会从失败中吸取教训。比如我们来算一下根号7是多少。一开始我们猜是2,2^2是4小了，就往大了猜，猜3 一看又大了。那猜2.5 发现又小了。不断根据误差更新自己的猜测。最终我们猜出了一个精确度可以忍受的答案。就如同大多数人的理想型都牵着别人的手（我比较幸运）。 好我们也模拟一下这个过程。
比如 我们一顿算，损失函数说我们多猜了1000. 那这多出了这1000 是哪多的呢？ 谁来背这个锅？首当其冲的当然是激活函数。 这里会用到导数，导数描述了一个函数的输出是如何根据输入量的变化而变化的。 当然是每个权重都有份。怎么分最合理呢？当然是按照权重比分。比如一个权重是1 一个是3.那很明显3的那个权重是老鼠屎的可能性最大。着重铲它。虽然挺美好的。但是计算挺繁琐的，wi/Ew 算每个权重，而且每次占比都要重新计算，所以就取巧，计算Wt。通常来说，在更新矩阵前会把误差乘以一个相对小的系数（learning_rate) 防止权重更新过大导致的震荡。 更新每一层都是同样的原理。


